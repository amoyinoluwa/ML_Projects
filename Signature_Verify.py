# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qGu_bWr9K08PAJzOt7mMgMrJOom0v334
"""

'''
The goal of this project to build a model that check whether sample signatures are from the same writer. 
Using a Kaggle dataset that consists of 2600 real and forged signatures of different people, I trained the model in Google colab using the 
TensorFlow library and achieved an accuracy of 46.7%. Unlike online signature verification
which has many proposed solutions, Offline signature verification is still an active research area. Currently, there is no algorithm that has 
completely solved the problem due to lack of data and discrepancies in individual signatures. I determined that the accuracy may 
improve with a larger dataset.
'''
import tensorflow as tf
from tensorflow.keras import datasets, layers, models, Model, optimizers
from tensorflow.keras.layers import Lambda, Dropout, Conv2D, Input, MaxPooling2D, Flatten, Dense, BatchNormalization, Activation
from tensorflow.keras.models import Sequential
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard
from tensorflow.keras.optimizers import SGD
import matplotlib.pyplot as plt
from sklearn.utils import shuffle
import glob
import numpy as np
import pandas as pd
import os
import cv2
import datetime
from tensorflow.keras import backend as K
from google.colab.patches import cv2_imshow

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/gdrive')
# %cd /content/gdrive/My Drive/

# Commented out IPython magic to ensure Python compatibility.
# %ls

dir = '/content/gdrive/My Drive/sign_data/'
currDir = os.listdir(dir)
trainLabels = os.path.join(dir, currDir[1])
testLabels = os.path.join(dir, currDir[0])
testDir = os.path.join(dir, currDir[2])
trainDir = os.path.join(dir, currDir[3])

def read_data(dir, data):
    '''
    Method to read in the image data and append to labels
    '''
    images1 = []  #list to hold real images
    images2 = []  #list to hold forged images
    labels = []
    for j in range(0, len(data)):
        path = os.path.join(dir, data.iat[j, 0]) #join directory and image name
        if os.path.exists(path):
            img1 = cv2.imread(path)
        else:
            print("Path does not exist:", path) 
            return
        img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY) #convert image to grayscale
        img1 = cv2.resize(img1, (128, 128)) #resize image
        images1.append([img1]) #add image to list
        path = os.path.join(dir, data.iat[j, 1])
        if os.path.exists(path):
            img2 = cv2.imread(path)
        else:
            print("Path does not exist:", path)
            return
        img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)
        img2 = cv2.resize(img2, (128, 128))
        images2.append([img2])
        labels.append(np.array(data.iat[j, 2]))
    images1 = np.array(images1).astype(np.float32) / 255.0 #normalize image pixels to a value between 0 and 1.
    images2 = np.array(images2).astype(np.float32) / 255.0
    labels = np.array(labels).astype(np.float32)
    return images1, images2, labels

def euclidean_distance(vects):
    x, y = vects
    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))

def eucl_dist_output_shape(shapes):
    shape1, shape2 = shapes
    return (shape1[0], 1)

df_train = pd.read_csv(trainLabels, header=None)
df_train.head()
df_test = pd.read_csv(testLabels, header=None)
df_test.head()
train_images1, train_images2, train_labels = read_data(dir=trainDir, data=df_train)
test_images1, test_images2, test_labels = read_data(dir=testDir, data=df_test)
train_images1, train_images2, train_labels = shuffle(train_images1, train_images2, train_labels)
test_images1, test_images2, test_labels = shuffle(test_images1, test_images2, test_labels)
size = 128
train_images1 = train_images1.reshape(-1, size, size, 1)
train_images2 = train_images2.reshape(-1, size, size, 1)
test_images1 = test_images1.reshape(-1, size, size, 1)
test_images2 = test_images2.reshape(-1, size, size, 1)

def build_model(input_shape):
    model = Sequential()
    model.add(Conv2D(64, (3, 3), input_shape=input_shape))
    model.add(Activation('relu'))
    model.add(BatchNormalization())
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(32, (3, 3)))
    model.add(Activation('relu'))
    model.add(BatchNormalization())
    model.add(MaxPooling2D((2, 2)))
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dense(64, activation='relu'))
    return model
    #model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])  
    #model.summary()
input_shape=(size,size,1)
base_network = build_model(input_shape)
img_a = Input(shape=input_shape)
img_b = Input(shape=input_shape)
feat_vecs_a = base_network(img_a)
feat_vecs_b = base_network(img_b)

distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([feat_vecs_a, feat_vecs_b])
prediction = Dense(1, activation='sigmoid')(distance)
earlyStopping_callback = EarlyStopping(monitor='val_loss',
                              min_delta=0,
                              patience=3,
                              verbose=1) 
logdir = os.path.join("logs", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir)

model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True, verbose=1)

callback_early_stop_reduceLROnPlateau = [earlyStopping_callback, tensorboard_callback, model_checkpoint_callback]

model = Model([img_a, img_b], prediction)
model.summary()

opt = SGD(learning_rate=0.00001)
model.compile(loss="binary_crossentropy", optimizer=opt, metrics=["accuracy"])
trainedModel = model.fit([train_images1, train_images2], train_labels, validation_split=0.2,
                    batch_size=128, verbose=1, epochs=10, callbacks=callback_early_stop_reduceLROnPlateau)

acc = trainedModel.history['accuracy']
val_acc = trainedModel.history['val_accuracy']

loss = trainedModel.history['loss']
val_loss = trainedModel.history['val_loss']

epochs_range = range(4)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

#predict on new data
pred = model.predict([test_images1[:1], test_images2[:1]]) #compare two signatures
val = np.argmax(pred, axis=1)
for i in val:
  if i == 0:
    print("The signatures are from the same writer")
    break
  else:
    print("The signatures are not from the same writer")
    break